{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "productive-accommodation",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stylish-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import tensorflow as tf, re, math\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras import backend as K \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-basin",
   "metadata": {},
   "source": [
    "### Checking for TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hearing-participation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not connected to a TPU runtime. Using CPU/GPU strategy\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "/usr/bin/sh: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-mount",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quarterly-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(root_file_name, filter_name):\n",
    "    events = uproot.open(root_file_name, filter_name=filter_name)[\"tree\"]\n",
    "    df = events.arrays(library=\"pd\")\n",
    "    return df\n",
    "\n",
    "features = []\n",
    "# variables: general\n",
    "features += ['FatJet_pt', 'FatJet_eta', 'FatJet_phi', 'FatJet_DDX_jetNSecondaryVertices', \\\n",
    "             'FatJet_DDX_jetNTracks', 'FatJet_DDX_z_ratio', 'FatJet_Proba', 'FatJet_area', \\\n",
    "             'FatJet_jetId', 'FatJet_lsf3', 'FatJet_rawFactor', 'FatJet_n2b1', 'FatJet_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau1\n",
    "features += ['FatJet_tau1', 'FatJet_DDX_tau1_flightDistance2dSig', 'FatJet_DDX_tau1_trackEtaRel_0', \\\n",
    "             'FatJet_DDX_tau1_trackEtaRel_1', 'FatJet_DDX_tau1_trackEtaRel_2', 'FatJet_DDX_tau1_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_tau1_trackSip3dSig_1', 'FatJet_DDX_tau1_vertexDeltaR', 'FatJet_DDX_tau1_vertexEnergyRatio', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau2\n",
    "features += ['FatJet_tau2', 'FatJet_DDX_tau2_flightDistance2dSig', 'FatJet_DDX_tau2_trackEtaRel_0', \\\n",
    "             'FatJet_DDX_tau2_trackEtaRel_1', 'FatJet_DDX_tau2_trackEtaRel_3', 'FatJet_DDX_tau2_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_tau2_trackSip3dSig_1', 'FatJet_DDX_tau2_vertexEnergyRatio', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau3 and tau4\n",
    "features += ['FatJet_tau3', 'FatJet_tau4',]\n",
    "\n",
    "# variables: track\n",
    "features += ['FatJet_DDX_trackSip2dSigAboveBottom_0', 'FatJet_DDX_trackSip2dSigAboveBottom_1', \\\n",
    "             'FatJet_DDX_trackSip2dSigAboveCharm', 'FatJet_DDX_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_trackSip3dSig_1', 'FatJet_DDX_trackSip3dSig_2', 'FatJet_DDX_trackSip3dSig_3', \\\n",
    "            ]\n",
    "\n",
    "# variables: subjet 1\n",
    "features += ['FatJet_subjet1_pt', 'FatJet_subjet1_eta', 'FatJet_subjet1_phi', \\\n",
    "             'FatJet_subjet1_Proba', 'FatJet_subjet1_tau1', 'FatJet_subjet1_tau2', \\\n",
    "             'FatJet_subjet1_tau3', 'FatJet_subjet1_tau4', 'FatJet_subjet1_n2b1', 'FatJet_subjet1_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: subjet 2\n",
    "features += ['FatJet_subjet2_pt', 'FatJet_subjet2_eta', 'FatJet_subjet2_phi', \\\n",
    "             'FatJet_subjet2_Proba', 'FatJet_subjet2_tau1', 'FatJet_subjet2_tau2', \\\n",
    "             'FatJet_subjet2_tau3', 'FatJet_subjet2_tau4', 'FatJet_subjet2_n2b1', 'FatJet_subjet2_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: fatjet sv\n",
    "features += ['FatJet_sv_costhetasvpv', 'FatJet_sv_d3dsig', 'FatJet_sv_deltaR', 'FatJet_sv_dxysig', \\\n",
    "             'FatJet_sv_enration', 'FatJet_sv_normchi2', 'FatJet_sv_ntracks', 'FatJet_sv_phirel', \\\n",
    "             'FatJet_sv_pt', 'FatJet_sv_ptrel', \\\n",
    "            ]\n",
    "\n",
    "features = sorted(features)\n",
    "\n",
    "# root_dir = \"/eos/user/a/afriberg/datasets/QCD_samples/\"\n",
    "\n",
    "# dirs = os.listdir(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-ground",
   "metadata": {},
   "source": [
    "### This gets QCD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "revised-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_file = dirs.pop(0)\n",
    "# while \".root\" not in first_file:\n",
    "#     first_file = dirs.pop(0)\n",
    "\n",
    "# first_file = root_dir + first_file\n",
    "# df = get_df(first_file, '*')\n",
    "# # Select a particular type of particle 0 means QCD\n",
    "# df.query(\"FatJet_gen_hadronFlavour == 0\", inplace=True)\n",
    "# df.dropna(inplace=True)\n",
    "# df = df[features]\n",
    "# # Prior to this, df is a pandas dataframe\n",
    "# X = df.to_numpy().astype(np.float32)\n",
    "# print(np.shape(X))\n",
    "\n",
    "\n",
    "# for inputfile in dirs:\n",
    "#     if \".root\" not in inputfile:\n",
    "#         continue\n",
    "#     inputfile = root_dir + inputfile\n",
    "#     df = get_df(inputfile, '*')\n",
    "#     df.dropna(inplace=True)\n",
    "#     df = df[features]\n",
    "#     # Prior to this, df is a pandas dataframe\n",
    "#     next_data = df.to_numpy().astype(np.float32)\n",
    "#     print(f\"next data has shape {np.shape(next_data)}\")\n",
    "#     # appending it to the whole thing\n",
    "#     X = np.append(X, next_data, axis=0)\n",
    "\n",
    "# print(f\"X has shape {np.shape(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-tolerance",
   "metadata": {},
   "source": [
    "# Getting HToBB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hollow-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = \"/eos/user/a/afriberg/datasets/has_B/ZH_HToBB_ZToLL_M125_13TeV_powheg_pythia8.root\"\n",
    "df = get_df(new_path, '*')\n",
    "# hadron flavour of 4 means Charm, 5 means B quark. Check the paper David referenced for more information\n",
    "# Getting bottom quark data\n",
    "b_quarks = df.query(\"FatJet_gen_hadronFlavour == 5\", inplace=False)\n",
    "b_quarks = b_quarks.dropna(inplace=False)\n",
    "b_quarks = b_quarks[features]\n",
    "\n",
    "original_dim = len(b_quarks.dtypes)\n",
    "\n",
    "b_quarks[\"is_B_quark\"] = 1\n",
    "# b_quarks.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-valve",
   "metadata": {},
   "source": [
    "#### Doing the same for charm quark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coastal-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting charm quark data\n",
    "c_quarks = df.query(\"FatJet_gen_hadronFlavour == 4\", inplace=False)\n",
    "c_quarks = c_quarks.dropna(inplace=False)\n",
    "c_quarks = c_quarks[features]\n",
    "\n",
    "c_quarks[\"is_B_quark\"] = 0\n",
    "\n",
    "c_X = c_quarks.to_numpy()\n",
    "c_X = c_X.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brave-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quarks = pd.concat([c_quarks, b_quarks])\n",
    "# all_quarks.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-pharmacology",
   "metadata": {},
   "source": [
    "#### Turning it into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "israeli-stephen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25353, 70)\n"
     ]
    }
   ],
   "source": [
    "X = all_quarks.to_numpy()\n",
    "X = X.astype(\"float32\")\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "limited-trigger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FatJet_DDX_jetNSecondaryVertices</th>\n",
       "      <th>FatJet_DDX_jetNTracks</th>\n",
       "      <th>FatJet_DDX_tau1_flightDistance2dSig</th>\n",
       "      <th>FatJet_DDX_tau1_trackEtaRel_0</th>\n",
       "      <th>FatJet_DDX_tau1_trackEtaRel_1</th>\n",
       "      <th>FatJet_DDX_tau1_trackEtaRel_2</th>\n",
       "      <th>FatJet_DDX_tau1_trackSip3dSig_0</th>\n",
       "      <th>FatJet_DDX_tau1_trackSip3dSig_1</th>\n",
       "      <th>FatJet_DDX_tau1_vertexDeltaR</th>\n",
       "      <th>FatJet_DDX_tau1_vertexEnergyRatio</th>\n",
       "      <th>...</th>\n",
       "      <th>FatJet_sv_normchi2</th>\n",
       "      <th>FatJet_sv_ntracks</th>\n",
       "      <th>FatJet_sv_phirel</th>\n",
       "      <th>FatJet_sv_pt</th>\n",
       "      <th>FatJet_sv_ptrel</th>\n",
       "      <th>FatJet_tau1</th>\n",
       "      <th>FatJet_tau2</th>\n",
       "      <th>FatJet_tau3</th>\n",
       "      <th>FatJet_tau4</th>\n",
       "      <th>is_B_quark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.000000</td>\n",
       "      <td>23350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.382441</td>\n",
       "      <td>10.302484</td>\n",
       "      <td>16.729216</td>\n",
       "      <td>2.335305</td>\n",
       "      <td>2.955847</td>\n",
       "      <td>2.219837</td>\n",
       "      <td>6.745109</td>\n",
       "      <td>-8.904252</td>\n",
       "      <td>-0.101879</td>\n",
       "      <td>1.826218</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.020085</td>\n",
       "      <td>-2.985353</td>\n",
       "      <td>-6.318169</td>\n",
       "      <td>50.354596</td>\n",
       "      <td>-6.085274</td>\n",
       "      <td>0.194115</td>\n",
       "      <td>0.085310</td>\n",
       "      <td>0.063662</td>\n",
       "      <td>0.052793</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.038073</td>\n",
       "      <td>4.440896</td>\n",
       "      <td>26.264698</td>\n",
       "      <td>1.759065</td>\n",
       "      <td>2.139291</td>\n",
       "      <td>2.679875</td>\n",
       "      <td>33.228586</td>\n",
       "      <td>23.398584</td>\n",
       "      <td>0.429933</td>\n",
       "      <td>4.994916</td>\n",
       "      <td>...</td>\n",
       "      <td>24.612635</td>\n",
       "      <td>25.119014</td>\n",
       "      <td>24.215512</td>\n",
       "      <td>60.161516</td>\n",
       "      <td>24.276335</td>\n",
       "      <td>0.129628</td>\n",
       "      <td>0.047741</td>\n",
       "      <td>0.031345</td>\n",
       "      <td>0.026282</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-190.750000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.529297</td>\n",
       "      <td>1.796875</td>\n",
       "      <td>2.214844</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.648438</td>\n",
       "      <td>-5.702148</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>0.129639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-0.015068</td>\n",
       "      <td>19.812500</td>\n",
       "      <td>0.080261</td>\n",
       "      <td>0.083328</td>\n",
       "      <td>0.052406</td>\n",
       "      <td>0.041168</td>\n",
       "      <td>0.033691</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.863281</td>\n",
       "      <td>2.728516</td>\n",
       "      <td>3.431641</td>\n",
       "      <td>3.123047</td>\n",
       "      <td>2.243164</td>\n",
       "      <td>0.528809</td>\n",
       "      <td>0.019791</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>...</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>47.968750</td>\n",
       "      <td>0.219604</td>\n",
       "      <td>0.155640</td>\n",
       "      <td>0.074097</td>\n",
       "      <td>0.058472</td>\n",
       "      <td>0.048615</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>22.792969</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>4.323242</td>\n",
       "      <td>4.371094</td>\n",
       "      <td>12.171875</td>\n",
       "      <td>1.973633</td>\n",
       "      <td>0.081482</td>\n",
       "      <td>1.752930</td>\n",
       "      <td>...</td>\n",
       "      <td>1.420898</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.041382</td>\n",
       "      <td>89.125000</td>\n",
       "      <td>0.400635</td>\n",
       "      <td>0.290283</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.080566</td>\n",
       "      <td>0.067749</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>6.886719</td>\n",
       "      <td>9.484375</td>\n",
       "      <td>9.007812</td>\n",
       "      <td>1185.000000</td>\n",
       "      <td>288.500000</td>\n",
       "      <td>0.864258</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>58.781250</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.781738</td>\n",
       "      <td>241.875000</td>\n",
       "      <td>0.735840</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.386475</td>\n",
       "      <td>0.272461</td>\n",
       "      <td>0.201416</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FatJet_DDX_jetNSecondaryVertices  FatJet_DDX_jetNTracks  \\\n",
       "count                      23350.000000           23350.000000   \n",
       "mean                           1.382441              10.302484   \n",
       "std                            1.038073               4.440896   \n",
       "min                            0.000000               0.000000   \n",
       "25%                            1.000000               7.000000   \n",
       "50%                            1.000000              10.000000   \n",
       "75%                            2.000000              13.000000   \n",
       "max                            8.000000              35.000000   \n",
       "\n",
       "       FatJet_DDX_tau1_flightDistance2dSig  FatJet_DDX_tau1_trackEtaRel_0  \\\n",
       "count                         23350.000000                   23350.000000   \n",
       "mean                             16.729216                       2.335305   \n",
       "std                              26.264698                       1.759065   \n",
       "min                            -190.750000                      -1.000000   \n",
       "25%                               2.529297                       1.796875   \n",
       "50%                               7.863281                       2.728516   \n",
       "75%                              22.792969                       3.562500   \n",
       "max                             525.000000                       6.886719   \n",
       "\n",
       "       FatJet_DDX_tau1_trackEtaRel_1  FatJet_DDX_tau1_trackEtaRel_2  \\\n",
       "count                   23350.000000                   23350.000000   \n",
       "mean                        2.955847                       2.219837   \n",
       "std                         2.139291                       2.679875   \n",
       "min                        -1.000000                      -1.000000   \n",
       "25%                         2.214844                      -1.000000   \n",
       "50%                         3.431641                       3.123047   \n",
       "75%                         4.323242                       4.371094   \n",
       "max                         9.484375                       9.007812   \n",
       "\n",
       "       FatJet_DDX_tau1_trackSip3dSig_0  FatJet_DDX_tau1_trackSip3dSig_1  \\\n",
       "count                     23350.000000                     23350.000000   \n",
       "mean                          6.745109                        -8.904252   \n",
       "std                          33.228586                        23.398584   \n",
       "min                         -50.000000                       -50.000000   \n",
       "25%                           0.648438                        -5.702148   \n",
       "50%                           2.243164                         0.528809   \n",
       "75%                          12.171875                         1.973633   \n",
       "max                        1185.000000                       288.500000   \n",
       "\n",
       "       FatJet_DDX_tau1_vertexDeltaR  FatJet_DDX_tau1_vertexEnergyRatio  ...  \\\n",
       "count                  23350.000000                       23350.000000  ...   \n",
       "mean                      -0.101879                           1.826218  ...   \n",
       "std                        0.429933                           4.994916  ...   \n",
       "min                       -1.000000                          -1.000000  ...   \n",
       "25%                        0.005841                           0.129639  ...   \n",
       "50%                        0.019791                           0.625488  ...   \n",
       "75%                        0.081482                           1.752930  ...   \n",
       "max                        0.864258                          50.000000  ...   \n",
       "\n",
       "       FatJet_sv_normchi2  FatJet_sv_ntracks  FatJet_sv_phirel  FatJet_sv_pt  \\\n",
       "count        23350.000000       23350.000000      23350.000000  23350.000000   \n",
       "mean            -5.020085          -2.985353         -6.318169     50.354596   \n",
       "std             24.612635          25.119014         24.215512     60.161516   \n",
       "min            -99.000000         -99.000000        -99.000000    -99.000000   \n",
       "25%              0.281250           2.000000         -0.015068     19.812500   \n",
       "50%              1.125000           3.000000          0.016129     47.968750   \n",
       "75%              1.420898           4.000000          0.041382     89.125000   \n",
       "max             58.781250           7.000000          0.781738    241.875000   \n",
       "\n",
       "       FatJet_sv_ptrel   FatJet_tau1   FatJet_tau2   FatJet_tau3  \\\n",
       "count     23350.000000  23350.000000  23350.000000  23350.000000   \n",
       "mean         -6.085274      0.194115      0.085310      0.063662   \n",
       "std          24.276335      0.129628      0.047741      0.031345   \n",
       "min         -99.000000      0.002764      0.000914      0.000000   \n",
       "25%           0.080261      0.083328      0.052406      0.041168   \n",
       "50%           0.219604      0.155640      0.074097      0.058472   \n",
       "75%           0.400635      0.290283      0.105469      0.080566   \n",
       "max           0.735840      0.640625      0.386475      0.272461   \n",
       "\n",
       "        FatJet_tau4  is_B_quark  \n",
       "count  23350.000000     23350.0  \n",
       "mean       0.052793         1.0  \n",
       "std        0.026282         0.0  \n",
       "min        0.000000         1.0  \n",
       "25%        0.033691         1.0  \n",
       "50%        0.048615         1.0  \n",
       "75%        0.067749         1.0  \n",
       "max        0.201416         1.0  \n",
       "\n",
       "[8 rows x 70 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_quarks.describe()\n",
    "# c_quarks.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-affect",
   "metadata": {},
   "source": [
    "This scales the HToBB data and the QCD data together, and then splits them in two parts afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sixth-excuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has shape (25353, 70)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(X)\n",
    "print(f\"data has shape {np.shape(data)}\")\n",
    "# Testing the normalized features\n",
    "# test = pd.DataFrame(data, columns=features + [\"is_B_quark\"])\n",
    "# test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-diameter",
   "metadata": {},
   "source": [
    "## Run this regardless of presence or lack of HToBB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "after-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing = train_test_split(data, test_size=0.2)\n",
    "x_train = training[:, :original_dim]\n",
    "y_train = training[:, original_dim]\n",
    "\n",
    "x_test = testing[:, :original_dim]\n",
    "y_test = testing[:, original_dim]\n",
    "# print(train_test_split(data, test_size=0.20)[:69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pleased-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def build_dset(df): \n",
    "    df = df.copy()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((df, df))\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "    \n",
    "x_train_dataset = build_dset(x_train)\n",
    "x_test_dataset = build_dset(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-graduate",
   "metadata": {},
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unlikely-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-cargo",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "moving-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(original_dim, latent_dim):\n",
    "    # Encoder\n",
    "    encoder_inputs = layers.Input(shape=(original_dim,))\n",
    "    h = layers.Dense(32, activation='relu')(encoder_inputs)\n",
    "    h = layers.Dense(16, activation='relu')(h)\n",
    "    h = layers.Dense(8, activation='relu')(h)\n",
    "    h = layers.Dense(latent_dim, activation='sigmoid')(h)\n",
    "    z_mu = layers.Dense(latent_dim, name=\"z_mean\")(h)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(h)\n",
    "    z = Sampling()([z_mu, z_log_var])\n",
    "    \n",
    "    encoder = Model(encoder_inputs, [z_mu, z_log_var, z], name=\"encoder\")\n",
    "    return encoder\n",
    "    \n",
    "def get_decoder(original_dim, latent_dim):\n",
    "    decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "    d = layers.Dense(8, activation='relu')(decoder_inputs)\n",
    "    d = layers.Dense(16, activation='relu')(d)\n",
    "    d = layers.Dense(32, activation='relu')(d)\n",
    "    d = layers.Dense(original_dim, activation='relu')(d)\n",
    "    \n",
    "    decoder = Model(decoder_inputs, d, name=\"decoder\")\n",
    "    return decoder\n",
    "\n",
    "class vae(Model):\n",
    "    def __init__(self, encoder, decoder, verbose=True, **kwargs):\n",
    "        super(vae, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "        if verbose:\n",
    "            self.encoder.summary()\n",
    "            self.decoder.summary()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf.keras.losses.binary_crossentropy(data, reconstruction), axis=-1\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def call(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        y_pred = self.decoder(z)\n",
    "        return y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-spirit",
   "metadata": {},
   "source": [
    "### Building a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "excess-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary(Model):\n",
    "    # Takes in a PRE-TRAINED encoder. This is just a test, mostly\n",
    "    def __init__(self, encoder, **kwargs):\n",
    "        super(Binary, self).__init__(**kwargs)\n",
    "        # Making a copy because we don't want to mess with the original\n",
    "        self.encoder = tf.keras.models.clone_model(encoder)\n",
    "        \n",
    "        latent_length = self.encoder.layers[-1].output_shape[-1]\n",
    "        for layer in self.encoder.layers:\n",
    "            print(layer)\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Some stuff that I honestly don't understand very well\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "        \n",
    "        # Creating a few classification layers\n",
    "        class_inputs = layers.Input(shape=(latent_length,))\n",
    "        h = layers.Dense(2, activation='relu')(class_inputs)\n",
    "        h = layers.Dense(1, activation='sigmoid')(h)\n",
    "        \n",
    "        self.classifier = Model(class_inputs, h, name=\"classifier\")\n",
    "            \n",
    "    def call(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        y_pred = self.classifier(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-floor",
   "metadata": {},
   "source": [
    "### JK we're making it a method cuz I don't know how classes work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "clear-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary(latent_dim):\n",
    "    input_layer = layers.Input(shape=(latent_dim,))\n",
    "    h = layers.Dense(2, activation='relu')(input_layer)\n",
    "    h = layers.Dense(1, activation='sigmoid')(h)\n",
    "    binary =  Model(input_layer, h, name=\"classifier\")\n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "altered-czech",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latent_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7c03608d106a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# We have to get the output from the VAE to use in this model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_x_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latent_dim' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = get_binary(latent_dim)\n",
    "\n",
    "# We have to get the output from the VAE to use in this model\n",
    "encoder = model.encoder\n",
    "z_mean, z_log_var, bin_x_train = encoder.predict(x_train)\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # parameters set to default values from Kingma and Ba paper.\n",
    "\n",
    "print(np.shape(bin_x_train))\n",
    "classifier.fit(bin_x_train, y_train, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-blackberry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "governing-crystal",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback():\n",
    "    lr_start   = 0.000001\n",
    "    lr_max     = 0.01\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 10\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max    \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback\n",
    "\n",
    "checkpoint_path = \"checkpoints/roc_weights.{epoch:05d}.hdf5\"\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 monitor = 'val_loss',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=False,\n",
    "                                                 mode = 'min',\n",
    "                                                 verbose=1)\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_dataset,\n",
    "    shuffle=True,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[cp_callback]\n",
    "    #callbacks=[cp_callback, get_lr_callback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-turning",
   "metadata": {},
   "source": [
    "# Loading a full VAE (or maybe AE) from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "encoder = get_encoder(original_dim, latent_dim)\n",
    "decoder = get_decoder(original_dim, latent_dim)\n",
    "model = vae(encoder, decoder)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1.e-3))\n",
    "model.evaluate(x_test)\n",
    "model.load_weights(\"working_QCD_checkpoints/tsg_vae weights.00020.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-foundation",
   "metadata": {},
   "source": [
    "# Plotting the test data errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(x_test)\n",
    "err = np.mean(np.abs(predict - x_test), axis=1)\n",
    "print(np.shape(err))\n",
    "\n",
    "bins = np.linspace(0, 0.4, 1000)\n",
    "# plt.hist(err, density=True)\n",
    "plt.hist(err, density=True, bins=bins)\n",
    "plt.xlabel(\"Mean Absolute Error\")\n",
    "plt.ylabel(\"Number of events (density)\")\n",
    "# Getting the name of the file we ran on\n",
    "plt.title(\"Aggregate QCD Data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-orlando",
   "metadata": {},
   "source": [
    "# Creating and training our binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-ranking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
