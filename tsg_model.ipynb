{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incoming-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import uproot\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-december",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "objective-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(root_file_name, filter_name):\n",
    "    events = uproot.open(root_file_name, filter_name=filter_name)[\"tree\"]\n",
    "    df = events.arrays(library=\"pd\")\n",
    "    return df\n",
    "\n",
    "features = []\n",
    "# variables: general\n",
    "features += ['FatJet_pt', 'FatJet_eta', 'FatJet_phi', 'FatJet_DDX_jetNSecondaryVertices', \\\n",
    "             'FatJet_DDX_jetNTracks', 'FatJet_DDX_z_ratio', 'FatJet_Proba', 'FatJet_area', \\\n",
    "             'FatJet_jetId', 'FatJet_lsf3', 'FatJet_rawFactor', 'FatJet_n2b1', 'FatJet_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau1\n",
    "features += ['FatJet_tau1', 'FatJet_DDX_tau1_flightDistance2dSig', 'FatJet_DDX_tau1_trackEtaRel_0', \\\n",
    "             'FatJet_DDX_tau1_trackEtaRel_1', 'FatJet_DDX_tau1_trackEtaRel_2', 'FatJet_DDX_tau1_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_tau1_trackSip3dSig_1', 'FatJet_DDX_tau1_vertexDeltaR', 'FatJet_DDX_tau1_vertexEnergyRatio', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau2\n",
    "features += ['FatJet_tau2', 'FatJet_DDX_tau2_flightDistance2dSig', 'FatJet_DDX_tau2_trackEtaRel_0', \\\n",
    "             'FatJet_DDX_tau2_trackEtaRel_1', 'FatJet_DDX_tau2_trackEtaRel_3', 'FatJet_DDX_tau2_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_tau2_trackSip3dSig_1', 'FatJet_DDX_tau2_vertexEnergyRatio', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau3 and tau4\n",
    "features += ['FatJet_tau3', 'FatJet_tau4',]\n",
    "\n",
    "# variables: track\n",
    "features += ['FatJet_DDX_trackSip2dSigAboveBottom_0', 'FatJet_DDX_trackSip2dSigAboveBottom_1', \\\n",
    "             'FatJet_DDX_trackSip2dSigAboveCharm', 'FatJet_DDX_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_trackSip3dSig_1', 'FatJet_DDX_trackSip3dSig_2', 'FatJet_DDX_trackSip3dSig_3', \\\n",
    "            ]\n",
    "\n",
    "# variables: subjet 1\n",
    "features += ['FatJet_subjet1_pt', 'FatJet_subjet1_eta', 'FatJet_subjet1_phi', \\\n",
    "             'FatJet_subjet1_Proba', 'FatJet_subjet1_tau1', 'FatJet_subjet1_tau2', \\\n",
    "             'FatJet_subjet1_tau3', 'FatJet_subjet1_tau4', 'FatJet_subjet1_n2b1', 'FatJet_subjet1_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: subjet 2\n",
    "features += ['FatJet_subjet2_pt', 'FatJet_subjet2_eta', 'FatJet_subjet2_phi', \\\n",
    "             'FatJet_subjet2_Proba', 'FatJet_subjet2_tau1', 'FatJet_subjet2_tau2', \\\n",
    "             'FatJet_subjet2_tau3', 'FatJet_subjet2_tau4', 'FatJet_subjet2_n2b1', 'FatJet_subjet2_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: fatjet sv\n",
    "features += ['FatJet_sv_costhetasvpv', 'FatJet_sv_d3dsig', 'FatJet_sv_deltaR', 'FatJet_sv_dxysig', \\\n",
    "             'FatJet_sv_enration', 'FatJet_sv_normchi2', 'FatJet_sv_ntracks', 'FatJet_sv_phirel', \\\n",
    "             'FatJet_sv_pt', 'FatJet_sv_ptrel', \\\n",
    "            ]\n",
    "\n",
    "features = sorted(features)\n",
    "\n",
    "# inputfile = 'QCD_HT500to700.root'\n",
    "inputfile = '/eos/user/a/afriberg/datasets/QCD_samples/QCD_HT700to1000.root'\n",
    "df = get_df(inputfile, '*')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df = df[features]\n",
    "\n",
    "# Prior to this, df is a pandas dataframe\n",
    "X = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "framed-aberdeen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This should be close to an integer: 7.0\n",
      "values are all close to integers: True\n",
      "Shape of design matrix: (399367, 69)\n",
      "Shape of zero matrix: (224166, 69)\n",
      "Shape of positive matrix: (175126, 69)\n",
      "Shape of negative matrix: (75, 69)\n",
      "Shape of the data is (175126, 68)\n"
     ]
    }
   ],
   "source": [
    "# MUST BE RUN BEFORE THE NORMALIZATION\n",
    "# Splitting the data based on the number of secondary vertices that it has\n",
    "# The index of the maximum number of secondary vertices\n",
    "X = X.astype(\"float32\")\n",
    "max_idx = np.argmax(X[:, 0])\n",
    "print(f\"This should be close to an integer: {X[max_idx, 0]}\")\n",
    "\n",
    "print(f\"values are all close to integers: {np.allclose(X[:,0] - X[:,0].astype(int), 0)}\")\n",
    "\n",
    "zero_idxs = X[:,0]==0\n",
    "pos_idxs = X[:,0] > 0\n",
    "neg_idxs = X[:,0] < 0\n",
    "\n",
    "X_pos = X[pos_idxs]\n",
    "X_zero = X[zero_idxs]\n",
    "X_neg = X[neg_idxs]\n",
    "\n",
    "# my_bins = range(0, 8)\n",
    "# plt.hist(X[X[:,0] >= 0], bins=my_bins)\n",
    "# plt.show()\n",
    "\n",
    "print(f\"Shape of design matrix: {np.shape(X)}\")\n",
    "print(f\"Shape of zero matrix: {np.shape(X_zero)}\")\n",
    "print(f\"Shape of positive matrix: {np.shape(X_pos)}\")\n",
    "print(f\"Shape of negative matrix: {np.shape(X_neg)}\")\n",
    "\n",
    "# We only want to run on data with 0 Secondary Vertices\n",
    "data = X_pos[:, 1:]\n",
    "print(f\"Shape of the data is {np.shape(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "later-writer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Scale our data using a MinMaxScaler that will scale\n",
    "# each number so that it will be between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "X_pos = scaler.fit_transform(X_pos)\n",
    "X_neg = scaler.fit_transform(X_neg)\n",
    "\n",
    "\n",
    "x_train, x_test = train_test_split(data, test_size=0.20)\n",
    "original_dim = np.size(data, axis=1)\n",
    "print(original_dim)\n",
    "\n",
    "def build_dset(df): \n",
    "    df = df.copy()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((df, df))\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "    \n",
    "x_train_dataset = build_dset(x_train)\n",
    "x_test_dataset = build_dset(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-emphasis",
   "metadata": {},
   "source": [
    "### Sampling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "neutral-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-benjamin",
   "metadata": {},
   "source": [
    "### Defining the Encoding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "convertible-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "\n",
    "inputs = keras.Input(original_dim)\n",
    "x = layers.Dense(32, activation='relu')(inputs)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"log_variance\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"mean\")(x)\n",
    "z = Sampling(latent_dim, name=\"sampling\")([z_mean, z_log_var])\n",
    "\n",
    "encoder = keras.Model(inputs, [z_mean, z_log_var, z], name=\"Encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-details",
   "metadata": {},
   "source": [
    "### Building the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "superb-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_input = keras.Input(latent_dim)\n",
    "x = layers.Dense(16, activation='relu')(latent_input)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "x = layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "decoder = keras.Model(latent_input, x, name=\"Decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-skating",
   "metadata": {},
   "source": [
    "### Defining the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "vertical-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        \n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    # This was replaced from the orginal by using Otto's model\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf.keras.losses.binary_crossentropy(data, reconstruction), axis=-1\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def call(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        y_pred = self.decoder(z)\n",
    "        return y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-lottery",
   "metadata": {},
   "source": [
    "### Compiling the model and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-grenada",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=\"adam\")\n",
    "# vae.fit(x_train_dataset, shuffle=True, epochs=20, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-abuse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
