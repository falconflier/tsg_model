{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "informed-explanation",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "timely-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf, re, math\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras import backend as K \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-behalf",
   "metadata": {},
   "source": [
    "### Checking for TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "arctic-option",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not connected to a TPU runtime. Using CPU/GPU strategy\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "/usr/bin/sh: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-night",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "conventional-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features():\n",
    "    return ['FatJet_pt', 'FatJet_eta', 'FatJet_phi', 'FatJet_DDX_jetNSecondaryVertices', 'FatJet_DDX_jetNTracks', 'FatJet_DDX_z_ratio', 'FatJet_Proba', 'FatJet_area', 'FatJet_jetId', 'FatJet_lsf3', 'FatJet_mass', 'FatJet_msoftdrop', 'FatJet_rawFactor', 'FatJet_n2b1', 'FatJet_n3b1', 'FatJet_tau1', 'FatJet_DDX_tau1_flightDistance2dSig', 'FatJet_DDX_tau1_trackEtaRel_0', 'FatJet_DDX_tau1_trackEtaRel_1', 'FatJet_DDX_tau1_trackEtaRel_2', 'FatJet_DDX_tau1_trackSip3dSig_0', 'FatJet_DDX_tau1_trackSip3dSig_1', 'FatJet_DDX_tau1_vertexDeltaR', 'FatJet_DDX_tau1_vertexEnergyRatio', 'FatJet_DDX_tau1_vertexMass', 'FatJet_tau2', 'FatJet_DDX_tau2_flightDistance2dSig', 'FatJet_DDX_tau2_trackEtaRel_0', 'FatJet_DDX_tau2_trackEtaRel_1', 'FatJet_DDX_tau2_trackEtaRel_3', 'FatJet_DDX_tau2_trackSip3dSig_0', 'FatJet_DDX_tau2_trackSip3dSig_1', 'FatJet_DDX_tau2_vertexEnergyRatio', 'FatJet_DDX_tau2_vertexMass', 'FatJet_tau3', 'FatJet_tau4', 'FatJet_DDX_trackSip2dSigAboveBottom_0', 'FatJet_DDX_trackSip2dSigAboveBottom_1', 'FatJet_DDX_trackSip2dSigAboveCharm', 'FatJet_DDX_trackSip3dSig_0', 'FatJet_DDX_trackSip3dSig_1', 'FatJet_DDX_trackSip3dSig_2', 'FatJet_DDX_trackSip3dSig_3', 'FatJet_subjet1_pt', 'FatJet_subjet1_eta', 'FatJet_subjet1_phi', 'FatJet_subjet1_Proba', 'FatJet_subjet1_mass', 'FatJet_subjet1_tau1', 'FatJet_subjet1_tau2', 'FatJet_subjet1_tau3', 'FatJet_subjet1_tau4', 'FatJet_subjet1_n2b1', 'FatJet_subjet1_n3b1', 'FatJet_subjet2_pt', 'FatJet_subjet2_eta', 'FatJet_subjet2_phi', 'FatJet_subjet2_Proba', 'FatJet_subjet2_mass', 'FatJet_subjet2_tau1', 'FatJet_subjet2_tau2', 'FatJet_subjet2_tau3', 'FatJet_subjet2_tau4', 'FatJet_subjet2_n2b1', 'FatJet_subjet2_n3b1', 'FatJet_hadronFlavour', 'FatJet_sv_costhetasvpv', 'FatJet_sv_d3dsig', 'FatJet_sv_deltaR', 'FatJet_sv_dxysig', 'FatJet_sv_enration', 'FatJet_sv_mass', 'FatJet_sv_normchi2', 'FatJet_sv_ntracks', 'FatJet_sv_phirel', 'FatJet_sv_pt', 'FatJet_sv_ptrel', 'FatJet_nFatJetPFCands', 'FatJet_pfcand_max_deltar', 'FatJet_pfcand_mean_deltar', 'FatJet_gen_pt', 'FatJet_gen_eta', 'FatJet_gen_phi', 'FatJet_gen_hadronFlavour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "talented-socket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376980, 69)\n",
      "next data has shape (353590, 69)\n",
      "next data has shape (340485, 69)\n",
      "next data has shape (74979, 69)\n",
      "next data has shape (361458, 69)\n",
      "next data has shape (399367, 69)\n",
      "X has shape (1906859, 69)\n"
     ]
    }
   ],
   "source": [
    "def get_df(root_file_name, filter_name):\n",
    "    events = uproot.open(root_file_name, filter_name=filter_name)[\"tree\"]\n",
    "    df = events.arrays(library=\"pd\")\n",
    "    return df\n",
    "\n",
    "features = []\n",
    "# variables: general\n",
    "features += ['FatJet_pt', 'FatJet_eta', 'FatJet_phi', 'FatJet_DDX_jetNSecondaryVertices', \\\n",
    "             'FatJet_DDX_jetNTracks', 'FatJet_DDX_z_ratio', 'FatJet_Proba', 'FatJet_area', \\\n",
    "             'FatJet_jetId', 'FatJet_lsf3', 'FatJet_rawFactor', 'FatJet_n2b1', 'FatJet_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau1\n",
    "features += ['FatJet_tau1', 'FatJet_DDX_tau1_flightDistance2dSig', 'FatJet_DDX_tau1_trackEtaRel_0', \\\n",
    "             'FatJet_DDX_tau1_trackEtaRel_1', 'FatJet_DDX_tau1_trackEtaRel_2', 'FatJet_DDX_tau1_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_tau1_trackSip3dSig_1', 'FatJet_DDX_tau1_vertexDeltaR', 'FatJet_DDX_tau1_vertexEnergyRatio', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau2\n",
    "features += ['FatJet_tau2', 'FatJet_DDX_tau2_flightDistance2dSig', 'FatJet_DDX_tau2_trackEtaRel_0', \\\n",
    "             'FatJet_DDX_tau2_trackEtaRel_1', 'FatJet_DDX_tau2_trackEtaRel_3', 'FatJet_DDX_tau2_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_tau2_trackSip3dSig_1', 'FatJet_DDX_tau2_vertexEnergyRatio', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau3 and tau4\n",
    "features += ['FatJet_tau3', 'FatJet_tau4',]\n",
    "\n",
    "# variables: track\n",
    "features += ['FatJet_DDX_trackSip2dSigAboveBottom_0', 'FatJet_DDX_trackSip2dSigAboveBottom_1', \\\n",
    "             'FatJet_DDX_trackSip2dSigAboveCharm', 'FatJet_DDX_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_trackSip3dSig_1', 'FatJet_DDX_trackSip3dSig_2', 'FatJet_DDX_trackSip3dSig_3', \\\n",
    "            ]\n",
    "\n",
    "# variables: subjet 1\n",
    "features += ['FatJet_subjet1_pt', 'FatJet_subjet1_eta', 'FatJet_subjet1_phi', \\\n",
    "             'FatJet_subjet1_Proba', 'FatJet_subjet1_tau1', 'FatJet_subjet1_tau2', \\\n",
    "             'FatJet_subjet1_tau3', 'FatJet_subjet1_tau4', 'FatJet_subjet1_n2b1', 'FatJet_subjet1_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: subjet 2\n",
    "features += ['FatJet_subjet2_pt', 'FatJet_subjet2_eta', 'FatJet_subjet2_phi', \\\n",
    "             'FatJet_subjet2_Proba', 'FatJet_subjet2_tau1', 'FatJet_subjet2_tau2', \\\n",
    "             'FatJet_subjet2_tau3', 'FatJet_subjet2_tau4', 'FatJet_subjet2_n2b1', 'FatJet_subjet2_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: fatjet sv\n",
    "features += ['FatJet_sv_costhetasvpv', 'FatJet_sv_d3dsig', 'FatJet_sv_deltaR', 'FatJet_sv_dxysig', \\\n",
    "             'FatJet_sv_enration', 'FatJet_sv_normchi2', 'FatJet_sv_ntracks', 'FatJet_sv_phirel', \\\n",
    "             'FatJet_sv_pt', 'FatJet_sv_ptrel', \\\n",
    "            ]\n",
    "\n",
    "features = sorted(features)\n",
    "\n",
    "root_dir = \"/eos/user/a/afriberg/datasets/QCD_samples/\"\n",
    "\n",
    "dirs = os.listdir(root_dir)\n",
    "\n",
    "first_file = dirs.pop(0)\n",
    "while \".root\" not in first_file:\n",
    "    first_file = dirs.pop(0)\n",
    "\n",
    "first_file = root_dir + first_file\n",
    "df = get_df(first_file, '*')\n",
    "df.dropna(inplace=True)\n",
    "df = df[features]\n",
    "# Prior to this, df is a pandas dataframe\n",
    "X = df.to_numpy().astype(np.float32)\n",
    "print(np.shape(X))\n",
    "\n",
    "\n",
    "for inputfile in dirs:\n",
    "    if \".root\" not in inputfile:\n",
    "        continue\n",
    "    inputfile = root_dir + inputfile\n",
    "    df = get_df(inputfile, '*')\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[features]\n",
    "    # Prior to this, df is a pandas dataframe\n",
    "    next_data = df.to_numpy().astype(np.float32)\n",
    "    print(f\"next data has shape {np.shape(next_data)}\")\n",
    "    # appending it to the whole thing\n",
    "    X = np.append(X, next_data, axis=0)\n",
    "\n",
    "print(f\"X has shape {np.shape(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-appearance",
   "metadata": {},
   "source": [
    "# Run this \n",
    "** *only* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale our data using a MinMaxScaler that will scale\n",
    "# each number so that it will be between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(X)\n",
    "\n",
    "x_train, x_test = train_test_split(data, test_size=0.20)\n",
    "original_dim = np.size(data, axis=1)\n",
    "print(original_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def build_dset(df): \n",
    "    df = df.copy()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((df, df))\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "    \n",
    "x_train_dataset = build_dset(x_train)\n",
    "x_test_dataset = build_dset(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-shelter",
   "metadata": {},
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-motel",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(original_dim, latent_dim):\n",
    "    # Encoder\n",
    "    encoder_inputs = layers.Input(shape=(original_dim,))\n",
    "    h = layers.Dense(32, activation='relu')(encoder_inputs)\n",
    "    h = layers.Dense(16, activation='relu')(h)\n",
    "    h = layers.Dense(8, activation='relu')(h)\n",
    "    h = layers.Dense(latent_dim, activation='sigmoid')(h)\n",
    "    z_mu = layers.Dense(latent_dim, name=\"z_mean\")(h)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(h)\n",
    "    z = Sampling()([z_mu, z_log_var])\n",
    "    \n",
    "    encoder = Model(encoder_inputs, [z_mu, z_log_var, z], name=\"encoder\")\n",
    "    return encoder\n",
    "    \n",
    "def get_decoder(original_dim, latent_dim):\n",
    "    decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "    d = layers.Dense(8, activation='relu')(decoder_inputs)\n",
    "    d = layers.Dense(16, activation='relu')(d)\n",
    "    d = layers.Dense(32, activation='relu')(d)\n",
    "    d = layers.Dense(original_dim, activation='relu')(d)\n",
    "    \n",
    "    decoder = Model(decoder_inputs, d, name=\"decoder\")\n",
    "    return decoder\n",
    "\n",
    "class vae(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(vae, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf.keras.losses.binary_crossentropy(data, reconstruction), axis=-1\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def call(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        y_pred = self.decoder(z)\n",
    "        return y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-argentina",
   "metadata": {},
   "source": [
    "### get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "\n",
    "with strategy.scope():\n",
    "    encoder = get_encoder(original_dim, latent_dim)\n",
    "    decoder = get_decoder(original_dim, latent_dim)\n",
    "    model = vae(encoder, decoder)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1.e-3))\n",
    "    #model.compile(optimizer=tf.keras.optimizers.RMSprop())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-iceland",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback():\n",
    "    lr_start   = 0.000001\n",
    "    lr_max     = 0.01\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 10\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max    \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback\n",
    "\n",
    "checkpoint_path = \"checkpoints/tsg_vae weights.{epoch:05d}.hdf5\"\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 monitor = 'val_loss',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=False,\n",
    "                                                 mode = 'min',\n",
    "                                                 verbose=1)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_dataset,\n",
    "    shuffle=True,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[cp_callback]\n",
    "    #callbacks=[cp_callback, get_lr_callback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test)\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "plt.plot(history.history[\"loss\"])\n",
    "# plt.plot(history.history[\"kl_loss\"])\n",
    "# plt.plot(history.history[\"reconstruction_loss\"])\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-artist",
   "metadata": {},
   "source": [
    "# Loading a model from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = get_encoder(original_dim, latent_dim)\n",
    "decoder = get_decoder(original_dim, latent_dim)\n",
    "model = vae(encoder, decoder)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1.e-3))\n",
    "model.evaluate(x_test)\n",
    "model.load_weights(\"checkpoints/tsg_vae weights.00020.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-custom",
   "metadata": {},
   "source": [
    "### Plotting the test data errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(x_test)\n",
    "err = np.mean(np.abs(predict - x_test), axis=1)\n",
    "print(np.shape(err))\n",
    "\n",
    "# There are two humps, so we're plotting those separately to visualize well\n",
    "first_peaks = []\n",
    "second_peaks = []\n",
    "for idx, val in enumerate(err):\n",
    "    if val < 0.025:\n",
    "        first_peaks.append(idx)\n",
    "    elif 0.033 <= val < 0.05:\n",
    "        second_peaks.append(idx)\n",
    "\n",
    "low_err = err[first_peaks]\n",
    "high_err = err[second_peaks]\n",
    "\n",
    "low_err_test = x_test[first_peaks]\n",
    "high_err_test = x_test[second_peaks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 0.4, 1000)\n",
    "# plt.hist(err, density=True)\n",
    "plt.hist(err, density=True, bins=bins)\n",
    "plt.xlabel(\"Mean Absolute Error\")\n",
    "plt.ylabel(\"Number of events (density)\")\n",
    "# Getting the name of the file we ran on\n",
    "plt.title(\"Aggregate QCD Data\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# There are two humps, so we're plotting those separately to visualize well\n",
    "plt.hist(low_err, density=True, bins=bins, alpha=0.5, label=\"Lower error data\")\n",
    "plt.hist(high_err, density=True, bins=bins, alpha=0.5, label=\"Higher error data\")\n",
    "plt.xlabel(\"Mean Absolute Error\")\n",
    "plt.ylabel(\"Number of events (density)\")\n",
    "# Getting the name of the file we ran on\n",
    "plt.title(\"Aggregate QCD Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_idx = features.index('FatJet_pt')\n",
    "eta_idx = features.index('FatJet_eta')\n",
    "phi_idx = features.index('FatJet_phi')\n",
    "\n",
    "print(features[pt_idx])\n",
    "print(features[eta_idx])\n",
    "print(features[phi_idx])\n",
    "\n",
    "# Getting the data to plot eta, phi, and pt for both high and low error data\n",
    "high_err_pt = high_err_test[:, pt_idx]\n",
    "low_err_pt = low_err_test[:, pt_idx]\n",
    "\n",
    "high_err_eta = high_err_test[:, eta_idx]\n",
    "low_err_eta = low_err_test[:, eta_idx]\n",
    "\n",
    "high_err_phi = high_err_test[:, phi_idx]\n",
    "low_err_phi = low_err_test[:, phi_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 0.6, 1000)\n",
    "plt.hist(high_err_pt, density=True, bins=bins, alpha=0.5, label=\"Lower error data\")\n",
    "plt.hist(low_err_pt, density=True, bins=bins, alpha=0.5, label=\"Higher error data\")\n",
    "plt.xlabel(\"Transverse Momentum\")\n",
    "plt.ylabel(\"Number of events (density)\")\n",
    "# Getting the name of the file we ran on\n",
    "plt.title(\"Aggregate QCD Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "bins = np.linspace(0.9, 1, 1000)\n",
    "plt.hist(high_err_eta, density=True, bins=bins, alpha=0.5, label=\"Lower error data\")\n",
    "plt.hist(low_err_eta, density=True, bins=bins, alpha=0.5, label=\"Higher error data\")\n",
    "plt.xlabel(\"Eta\")\n",
    "plt.ylabel(\"Number of events (density)\")\n",
    "# Getting the name of the file we ran on\n",
    "plt.title(\"Aggregate QCD Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# bins = np.linspace(0, 1, 1000)\n",
    "plt.hist(high_err_phi, density=True, bins=bins, alpha=0.5, label=\"Lower error data\")\n",
    "plt.hist(low_err_phi, density=True, bins=bins, alpha=0.5, label=\"Higher error data\")\n",
    "plt.xlabel(\"Phi\")\n",
    "plt.ylabel(\"Number of events (density)\")\n",
    "# Getting the name of the file we ran on\n",
    "plt.title(\"Aggregate QCD Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-parking",
   "metadata": {},
   "source": [
    "## Plotting Latent Space Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_encoder = model.encoder\n",
    "z_mean_high, z_log_var_high, z_high = trained_encoder.predict(high_err_test)\n",
    "z_mean_low, z_log_var_low, z_low = trained_encoder.predict(low_err_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(z_high))\n",
    "num_high = np.size(z_high, axis=0) // 10\n",
    "num_low = np.size(z_low, axis=0) // 10\n",
    "print(num_high)\n",
    "plt.scatter(z_high[:num_high, 0], z_high[:num_high, 1], s=1, alpha=0.5, label=\"high error data\")\n",
    "plt.scatter(z_low[:num_low, 0], z_low[:num_low, 1], s=1, alpha=0.5, label=\"low error data\")\n",
    "plt.title(\"latent space representation\")\n",
    "plt.xlabel(\"z_0\")\n",
    "plt.ylabel(\"z_1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-4, 4, 500)\n",
    "plt.hist(z_high[:, 0], density=True, bins=bins, alpha=0.4, label=\"high error data\")\n",
    "plt.hist(z_low[:, 0], density=True, bins=bins, alpha=0.4, label=\"low error data\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"z_0\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(z_high[:, 1], density=True, bins=bins, alpha=0.4, label=\"high error data\")\n",
    "plt.hist(z_low[:, 1], density=True, bins=bins, alpha=0.4, label=\"low error data\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"z_1\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = np.linspace(0, 0.6, 1000)\n",
    "for idx, feat in enumerate(features):\n",
    "    high_err_feat = high_err_test[:, idx]\n",
    "    low_err_feat = low_err_test[:, idx]\n",
    "    \n",
    "#     plt.hist(low_err_feat, density=True, bins=bins, alpha=0.5, label=\"Lower error data\")\n",
    "#     plt.hist(high_err_feat, density=True, bins=bins, alpha=0.5, label=\"Higher error data\")\n",
    "    plt.hist(low_err_feat, density=True, bins=200, alpha=0.5, label=\"Lower error data\")\n",
    "    plt.hist(high_err_feat, density=True, bins=200, alpha=0.5, label=\"Higher error data\")\n",
    "    plt.xlabel(feat)\n",
    "    plt.ylabel(\"Number of events (density)\")\n",
    "    # Getting the name of the file we ran on\n",
    "    plt.title(feat)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_features = []\n",
    "for feat in features:\n",
    "    if \"sv\" in feat:\n",
    "        sv_features.append(feat)\n",
    "        print(feat)\n",
    "\n",
    "print(len(sv_features))\n",
    "\n",
    "feat_name = sv_features[0]\n",
    "print(f\"\\n\\nWorking on {feat_name}\")\n",
    "idx = features.index(feat_name)\n",
    "high_err_feat = high_err_test[:, idx]\n",
    "low_err_feat = low_err_test[:, idx]\n",
    "\n",
    "# bins=200\n",
    "# bins=np.linspace(0, 1, 100)\n",
    "# bins=np.linspace(0.85, 1, 100)\n",
    "bins=np.linspace(0, 0.25, 100)\n",
    "bins=np.linspace(0, 0.05, 100)\n",
    "plt.hist(low_err_feat, density=True, bins=bins, alpha=0.5, label=\"Lower error data\")\n",
    "plt.hist(high_err_feat, density=True, bins=bins, alpha=0.5, label=\"Higher error data\")\n",
    "plt.xlabel(feat_name)\n",
    "plt.ylabel(\"Number of events (density)\")\n",
    "# Getting the name of the file we ran on\n",
    "plt.title(feat_name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-outline",
   "metadata": {},
   "source": [
    "# Checking the features of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(x_train)\n",
    "err = np.mean(np.abs(predict - x_train), axis=1)\n",
    "print(np.shape(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 0.4, 1000)\n",
    "plt.hist(err, bins=bins)\n",
    "plt.xlabel(\"Error (MAE)\")\n",
    "plt.ylabel(\"Number of Events\")\n",
    "plt.title(\"Training data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-bench",
   "metadata": {},
   "source": [
    "### Selecting a new dataset and plotting the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/eos/user/a/afriberg/datasets/vae_err/\"\n",
    "files = os.listdir(root_dir)\n",
    "\n",
    "# Not a good way to do it, would use a dictionary but they're annoying to iterate over\n",
    "err_names = []\n",
    "errors = []\n",
    "\n",
    "print(files)\n",
    "for new_file in files:\n",
    "    print(f\"now working on {new_file}\")\n",
    "    # inputfile = 'QCD_HT500to700.root'\n",
    "    df = get_df(root_dir + new_file, '*')\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[features]\n",
    "\n",
    "    new_X = df.to_numpy()\n",
    "    new_X = new_X.astype(\"float32\")\n",
    "\n",
    "    # Scale our data using a MinMaxScaler that will scale \n",
    "    # each number so that it will be between 0 and 1\n",
    "    scaler = MinMaxScaler()\n",
    "    new_data = scaler.fit_transform(new_X)\n",
    "    predict = model.predict(new_data)\n",
    "    err = np.mean(np.abs(predict - new_data), axis=1)\n",
    "    print(np.shape(err))\n",
    "    \n",
    "    \n",
    "    bins = np.linspace(0, 0.4, 1000)\n",
    "    # plt.hist(err, density=True)\n",
    "    plt.hist(err, density=True, bins=bins)\n",
    "    plt.xlabel(\"Mean Absolute Error\")\n",
    "    plt.ylabel(\"Number of events (density)\")\n",
    "    # Getting the name of the file we ran on\n",
    "    title = new_file.rpartition('/')[-1].rpartition('.')[0]\n",
    "    plt.title(title)\n",
    "#     plt.savefig(\"images/\" + title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    err_names.append(title)\n",
    "    errors.append(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-invention",
   "metadata": {},
   "source": [
    "# Data with even more categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/eos/user/a/afriberg/datasets/stripped/\"\n",
    "new_file = \"ZH_HToBB_ZToLL_M125_13TeV_powheg_pythia8.root\"\n",
    "df = get_df(root_dir + new_file, '*')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df = df[features]\n",
    "\n",
    "new_X = df.to_numpy()\n",
    "new_X = new_X.astype(\"float32\")\n",
    "\n",
    "# Scale our data using a MinMaxScaler that will scale \n",
    "# each number so that it will be between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "new_data = scaler.fit_transform(new_X)\n",
    "new_predict = model.predict(new_data)\n",
    "new_err = np.mean(np.abs(new_predict - new_data), axis=1)\n",
    "print(np.shape(err))    \n",
    "    \n",
    "bins = np.linspace(0, 0.4, 1000)\n",
    "# plt.hist(err, density=True)\n",
    "plt.hist(err, density=True, bins=bins)\n",
    "plt.xlabel(\"Mean Absolute Error\")\n",
    "plt.ylabel(\"Number of events (density)\")\n",
    "# Getting the name of the file we ran on\n",
    "title = new_file.rpartition('/')[-1].rpartition('.')[0]\n",
    "plt.title(title)\n",
    "# plt.savefig(\"images/\" + title)\n",
    "plt.show()\n",
    "    \n",
    "# err_names.append(title)\n",
    "# errors.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two humps, so we're plotting those separately to visualize well\n",
    "first_peaks = []\n",
    "second_peaks = []\n",
    "for idx, val in enumerate(new_err):\n",
    "    if val < 0.10:\n",
    "        first_peaks.append(idx)\n",
    "    elif 0.17 <= val < 0.22:\n",
    "        second_peaks.append(idx)\n",
    "\n",
    "new_low_err = err[first_peaks]\n",
    "new_high_err = err[second_peaks]\n",
    "\n",
    "new_low_err_test = x_test[first_peaks]\n",
    "new_high_err_test = x_test[second_peaks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(err, density=True)\n",
    "plt.hist(new_low_err_test, density=True, bins=bins)\n",
    "plt.hist(new_high_err_test, density=True, bins=bins)\n",
    "plt.xlabel(\"Mean Absolute Error\")\n",
    "plt.ylabel(\"Number of events (density)\")\n",
    "# Getting the name of the file we ran on\n",
    "title = new_file.rpartition('/')[-1].rpartition('.')[0]\n",
    "plt.title(title)\n",
    "# plt.savefig(\"images/\" + title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-bryan",
   "metadata": {},
   "source": [
    "# Latent space representation of the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_encoder = model.encoder\n",
    "new_z_mean, new_z_log_var, new_z = trained_encoder.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.7\n",
    "plt.scatter(new_z[:, 0], new_z[:, 1], s=1, alpha=alpha, label=title[:10])\n",
    "plt.scatter(z_high[:num_high, 0], z_high[:num_high, 1], s=1, alpha=alpha, label=\"high error data\")\n",
    "plt.scatter(z_low[:num_low, 0], z_low[:num_low, 1], s=1, alpha=alpha, label=\"low error data\")\n",
    "plt.title(\"latent space representation\")\n",
    "plt.xlabel(\"z_0\")\n",
    "plt.ylabel(\"z_1\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(new_z[:, 0], new_z[:, 1], s=1, alpha=alpha, label=title[:10])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.scatter(z_high[:num_high, 0], z_high[:num_high, 1], s=1, alpha=alpha, label=\"high error data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.scatter(z_low[:num_low, 0], z_low[:num_low, 1], s=1, alpha=alpha, label=\"low error data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-block",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
